---
title: "Musiques : histoire de genres"
title-block-banner: "images/ColorWheel-Vinyl_A-Side.png"
subtitle: "Originaux et rééditions, une dynamique de cycle de vie ? "
author: "Christophe Benavent"
institute: "Paris Dauphine - PSL"
date : "today"
toc: true
number-sections: true
number-depth: 2
format: html
editor: visual
execute:
  echo: true
  warning: false
  message: false
fig-align: "center"
code-fold: true
---

# Introduction

Dans cette étude on se concentre sur la question des styles. Le but est d'en donner un tableau général des évolutions et des variations. D'avoir quelques ordres de grandeurs en têtes, et de mieux cadrer les traitement ultérieurs qui porterons sur l'analyse de leurs cooccurences.

```{r 0O}
#tools

#packages
library(tidyverse)
library(ggrepel)
library(word2vec)
library(doc2vec)
library(fastcluster) #pour aller plus vite en hc
library(Rtsne)
library(scales)
library(zoo)
library(jtools)


# pour ggplot



breaks = c(1900, 1910, 1920,1930,1940,1950,
                                1960,1970,1980,1990,2000,2010,2020)


#  les couleurs des genres . Y-a-il une conventions ?
col_genre<-c("blue4","coral4","skyblue","purple" , "darkturquoise","darkgreen",
                    "orange1","orange3","dodgerblue4","darkred","black","deeppink2","limegreen","red3","grey")
                    
col_format=c("brown4",  "skyblue1", "skyblue2", "skyblue3","grey","cyan",  "grey30")


col_pays= c("black", "green3","green1","lightblue1", "yellow", "lightblue2", "orange1", "lightblue4", "dodgerblue3", "dodgerblue1", "dodgerblue2","green2","lightblue3", "white", "green4", "orange2", "orange3", "blue3", "blue4")


theme_set(theme_minimal())

```

# Préparation des données

Nours reprenons le fichier de l'étude précédente qui caractérise les releases, et nous chargeons celui des styles .

```{r 01}
t1<-Sys.time()
release<-read.csv("./dataarchives/release_work.csv")%>%
  select(-X)

style<- read_csv("dataarchives/release_style2.txt")%>% #pb encodage
  rename(id=1) %>%
  mutate(category="Style") %>% mutate(style=tolower(style), 
                                      style=str_replace_all(style," ",""),
                                      style=str_replace_all(style,"-",""))

genre<- read_csv("dataarchives/release_genre.csv")%>%
  mutate(genre=str_replace_all(genre,"[:punct:]",""),
         genre=str_replace_all(genre," ","")) %>%
  rename(id=1,style=genre) %>% 
  mutate(category="Genre")

t2<-Sys.time()
t=t2-t1

```

# Un premier aperçu

Pour un premier aperçu, deux questions symétriques :

-   Combien de styles par release, ce qui permet d'approfondir le questionnement sur le diversité
-   Combien de sorties par style, pour commencer à appréhender le champs.

## Combien de styles par release ?

L'accroissement continu peut trouver plusieurs explications

-   La tendance inexorable du métissage

-   La tendance des annotateurs à nuancer leur etiquettage.

L'anomalie de 65-80.

```{r 02}
foo<- style %>% 
  group_by(id)%>%
  summarise(n_style=n())

ggplot(foo,aes(x=n_style))+  labs(x="Nombre de styles", y="Nombre d'éditions")+
  geom_histogram(binwidth = 1, fill="skyblue3")+
  scale_fill_manual(values=c("skyblue3", "skyblue1")) +
#   scale_x_continuous(breaks = breaks, limits = c(1940, 2020))+
   scale_y_continuous(label=comma,trans="log10")+
scale_x_continuous(label=comma , breaks =c(10,20,30,40,50,60,70,80,90,100))

ggsave(filename="./images/genre10.jpeg", plot=last_plot(), width = 27, height = 18, units = "cm")

```

```{r 03}
foo2<- release%>%
  left_join(foo)%>%
  group_by(year,version)%>%
  summarise(n_style_m=mean(n_style, na.rm=TRUE),
            n_style_d=sd(n_style, na.rm=TRUE))
  


ggplot(foo2,aes(x=year,y=n_style_m,group=version))+  
  labs(x=NULL, y="Nombre de styles")+
  geom_ribbon(aes(ymin = n_style_m - n_style_d/40, 
                  ymax = n_style_m + n_style_d/40, 
                  fill=version), alpha=.5)+
  scale_fill_manual(values=c("skyblue3", "skyblue1")) +
#   scale_x_continuous(breaks = breaks, limits = c(1940, 2020))+
   scale_y_continuous(breaks=c(1.2,1.4, 1.6, 1.8, 2), limits = c(1.2,2.2))+
scale_x_continuous( breaks =breaks,limits = c(1940,2020))

ggsave(filename="./images/genre11.jpeg", plot=last_plot(), width = 27, height = 18, units = "cm")


```

## Combien de releases par style ?

```{r 04}
foo<- style %>% 
  group_by(style)%>%
  summarise(n_release=n())%>%
  arrange(desc(n_release))%>% 
    mutate(rank=rank(n_release, ties.method='random'),
           sum=cumsum(n_release))
           
ggplot(foo,aes(x=604-rank,y=sum))+  
  labs(x="rang des styles", y="Nombre d'éditions")+
  geom_area(fill="skyblue3")+
   scale_y_continuous(label=comma )+
scale_x_continuous()

ggsave(filename="./images/genre12.jpeg", plot=last_plot(), width = 27, height = 18, units = "cm")
```

```{r 05}

foo2<-foo %>%filter(n_release>20000)
library(ggwordcloud)
set.seed(42)

ggplot(foo2, aes(label = style, size = n_release)) +
  geom_text_wordcloud() + 
  theme_minimal()


ggsave(filename="./images/genre13.jpeg", plot=last_plot(), width = 27, height = 18, units = "cm")


```

Cette première représentation est confuse, si le pop-rock domine, rien ne dit vraiment des tendances.

# Vectorization

le but de la vectorization est de représenter chaque style, puis chaque édition, par un vecteur. Pour Augmenter l'information on va y ajouter les genres.

L'analyse est réduite au corpus des originaux pour saisir au plus prêt de la création les indications de style et pour une raison de calcul, on passe de 13 millions à 7 millions d'oeuvres dans le corpus.

On distingue genre et style par la majuscule.

Le calcul prend du temps, plusieurs heures sur 4 coeurs.

## les styles

Le modèle Word2vec va nous permettre d'obtenir les embeddings des 603 styles identifiés et des 15 genres. On garde les majuscules pour identifier les genre, les styles ont été mis en minuscule auparavant.

```{r 06}


text1<-rbind(genre,style) %>%
  left_join(release)%>%
  filter(version=="Original") %>%
  select(id,title, pays2, date, style,category)

vocab<-text1 %>%
  group_by(style)%>%
  summarise(n=n())
  
text2<-text1 %>%  
 group_by(id) %>% 
 summarise(description = paste(style, collapse = " "))

write.csv(text2, "./dataarchives/text2.csv")

#text3<- txt_clean_word2vec(text2, ascii = TRUE, alpha = FALSE, tolower = FALSE, trim = TRUE)

#on vectorise

t1<-Sys.time()

set.seed(123456789)
model <- word2vec(x = text2$description, 
                  type = "skip-gram", 
                  window = 8, 
                  dim = 100, 
                  iter = 100,
                  threads = 4L,
                  min_count = 10L,
                  verbose=TRUE)

t2<-Sys.time()

t_2= t2-t1


embedding <- as.matrix(model)

#on sauvegarde le model pour des usages ultérieurs
write.csv(embedding, "./dataarchives/embedding.csv")
path <- "./dataarchives/mymodel.bin"
write.word2vec(model, file = path)

```

## typologie et representation 2D

```{r 07}

model <- read.word2vec(path)
embedding <- as.matrix(model)


#test sur le genre Rock
lookslike <- predict(model, c("hiphop"), type = "nearest", top_n = 20)
lookslike



#on typologise des termes
library(fastcluster) #pour aller plus vite

k=25
distance<-as.dist(1 - cor(t(embedding)))
arbre <- hclust(distance, method = "ward.D2")
plot(arbre,  xlab = "", ylab = "", sub = "", axes = FALSE, hang = -1)
rect.hclust(arbre,k, border = "green3")
group<- as.data.frame(cutree(arbre, k = k))

group<- group %>% 
  rownames_to_column(var="style")%>%
  rename(group=2) %>%  
  left_join(vocab, by="style")


library(ggwordcloud)
ggplot(group, aes(label = style, size = log10(n), color=n)) +
  geom_text_wordcloud_area(area_corr = TRUE) +
  scale_size_area(max_size = 3) +
  facet_wrap(vars(group), ncol=5)

ggsave(filename="cluster_word.jpeg", plot=last_plot(), width = 27, height = 18, units = "cm")

tsne_out <- Rtsne(embedding,perplexity = 8, dim=2) # Run TSNE
tsne_out2<-as.data.frame(tsne_out$Y)

#lemma<-rownames(embedding)

tsne_out3<-cbind(tsne_out2,group) %>%
  left_join(vocab) %>% filter(n>4)
c25 <- c(
  "dodgerblue2", "#E31A1C", # red
  "green4",
  "#6A3D9A", # purple
  "#FF7F00", # orange
  "black", "gold1",
  "skyblue2", "#FB9A99", # lt pink
  "palegreen2",
  "#CAB2D6", # lt purple
  "#FDBF6F", # lt orange
  "gray70", "khaki2",
  "maroon", "orchid1", "deeppink1", "blue1", "steelblue4",
  "darkturquoise", "green1", "yellow4", "yellow3",
  "darkorange4", "brown"
)


library(ggrepel)
tsne_out3%>%
  ggplot(aes(x=V1, y=V2, label=style))+
  geom_text_repel(aes(label=ifelse(n>14,style,""),size=n,color=as.factor(group)),max.overlaps=20)+
    labs(title="Projection 2D du vocabulaire vectorisé des 110 articles 'NLP & Marketing' ",
       subtitle="Pipe : annot. syntax -> word2vec: 200 vecteurs-> hclus ->rtsne",
       x= NULL, y=NULL)+ 
  scale_color_manual(values=c25) +  
  theme(legend.position = "none") 

ggsave(filename="vector_word.jpeg", plot=last_plot(), width = 27, height = 18, units = "cm")

```

## Analyser les similarités des vecteurs

```{r 08 }
#titre<-UD %>%group_by
x      <- data.frame(doc_id           = text2$id, 
                     text             = text2$description, 
                     stringsAsFactors = FALSE)
#x$text <- txt_clean_word2vec(x$text, tolower=TRUE)
emb <- as.data.frame(doc2vec(model, x$text,  split = " ",type = "embedding"))%>%
  drop_na()


release_emb<-cbind(text2,emb)%>%left_join(release)

rm(text1)
rm(style)
rm(text2)
rm(release)
rm(genre)
rm(emb)
foo<- release_emb %>% 
  select(id, year, 3:102) %>% 
  group_by(year)%>%
  summarise(across(2:101, ~ mean(.x, na.rm = TRUE)))%>%
filter(year>1939 & year<2021)%>%column_to_rownames(var="year")

library(factoextra)
library(FactoMineR)

res.pca<-PCA(foo, ncp = 5, graph = TRUE)
eig.val <- get_eigenvalue(res.pca)
eig.val

fviz_pca_ind (res.pca, col.ind = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Évite le chevauchement de texte
             )

foo1<- embedding %>%
  rownames_to_column(var="style") %>% 
  pivot_longer
```

# Vectorisation par période

La question du temps a été abordées différentes solutions sont proposées

-    le compass

-   la restimation à partir de la solution t-1 -

-   Dans cette section on va refaire exercice précédent mais en prenant en compte le temps, on distinguera chaque token par sa temporalité, une période de 5 ans. Pour réaliser cette tâche on transforme les tokens avec un suffixe \*\_YYYY\* ( pas de ref)

Comment prendre en compte les corrélation inter temporelles ? Pour mesurer l'évolution d'un style une approche simple est de considérer que si les espace construits pour chaque période peuvent être différents, les relations entre les styles ne sont pas supposées se modifier. Une variation de la corrélation ( cosinus) entre deux style, peut être appréciée comme un changement de signification.

Cette idée permet une mise en oeuvre simple : on construit autant de modèles que de période de temps, au sein de chacun des période on calcule la matrice des cos, On examine ensuite l'évolution de ces cos dans le temps.

On pourra ainsi pour un style donné considérer son glissement (drift) en examinant quels sont les genre qui s'en rapproche et qui s'en éloigent. Cette approche permet de se débarrasser de la nécessité d'un espace commun à travers le temps, et de ne pas chercher la dérive d'un style dans uh espace de référence, mais les dérives des styles concommittants.
